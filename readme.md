# ğŸ§  Self-Reflective RAG with LangGraph & Streamlit

An intelligent **Question-Answering system with self-correction**.  
Unlike standard RAG (Retrieval-Augmented Generation) pipelines, this system **evaluates its own retrievals and answers**. If the retrieved documents are irrelevant or the generated answer is a hallucination, the system **automatically rewrites the query and retries**.

Built using **LangChain**, **LangGraph**, **ChromaDB**, and **Streamlit**.

---

# ğŸš€ Key Features

- ğŸ” **Self-Reflection Loop**  
  Detects hallucinations or irrelevant context and retries automatically.

- âœï¸ **Query Transformation**  
  Rewrites user queries to improve document retrieval quality.

- ğŸ“Š **Relevance Grading**  
  Uses an LLM to judge whether retrieved documents are relevant to the query.

- ğŸ§ª **Hallucination Detection**  
  Verifies that the generated answer is grounded in retrieved documents.

- ğŸ–¥ï¸ **Interactive Streamlit UI**  
  Displays answers along with the systemâ€™s internal reasoning flow.

---

## ğŸ› ï¸ Architecture

The system follows a **cyclic graph workflow** implemented using **LangGraph**:

```mermaid
graph TD
    A[Start] --> B[Retrieve Documents]
    B --> C{Grade Documents}
    C -- Relevant --> D[Generate Answer]
    C -- Irrelevant --> E[Transform Query]
    E --> B
    D --> F{Check Hallucination}
    F -- Grounded & Useful --> G[End / Show Answer]
    F -- Hallucinated / Not Useful --> E
````

### Workflow Steps

1. **Retrieve** â€“ Fetches documents using vector search
2. **Grade** â€“ Checks if documents are relevant
3. **Transform** â€“ Rewrites the query if retrieval is poor
4. **Generate** â€“ Produces an answer using grounded context
5. **Verify** â€“ Confirms the answer is factual and useful

---

## ğŸ“‹ Prerequisites

* Python **3.9+**
* An **OpenAI API Key**

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/self-reflective-rag.git
cd self-reflective-rag
```

### 2ï¸âƒ£ Create a Virtual Environment (Recommended)

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## âš™ï¸ Configuration

### ğŸ”‘ Environment Variables

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=sk-your-openai-api-key
```

### ğŸ“„ Add Your Data

* Place the PDF you want to chat with in the root directory
* Rename it to **`input.pdf`**
* The system will automatically ingest and index it on first run

---

## ğŸƒâ€â™‚ï¸ Usage

Run the Streamlit app:

```bash
streamlit run app.py
```

Access the UI at:

```
http://localhost:8501
```

---

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ app.py                 # Streamlit UI entry point
â”œâ”€â”€ main_app.py            # Workflow initialization & orchestration
â”œâ”€â”€ workflow_manager.py    # LangGraph workflow definition
â”œâ”€â”€ workflow_nodes.py      # Core logic (Retrieve, Generate, Grade)
â”œâ”€â”€ prompt_templates.py    # LLM prompts for grading & rewriting
â”œâ”€â”€ pdf_handler.py         # PDF text extraction
â”œâ”€â”€ text_processing.py     # Chunking & preprocessing logic
â”œâ”€â”€ vector_store.py        # ChromaDB setup and retrieval
â”œâ”€â”€ .env                   # API keys (not committed)
â”œâ”€â”€ input.pdf              # Source document
â””â”€â”€ requirements.txt       # Dependencies
```

---

## ğŸ”§ Customization

* **Model Selection**
  Change the model (e.g., GPT-4) in `prompt_templates.py`

* **Chunk Size**
  Modify `chunk_size` in `text_processing.py` (default: 250)

* **Retry Limit**
  Adjust `recursion_limit` in `app.py` (default: 15)

